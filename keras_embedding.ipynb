{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "https://github.com/fchollet/keras/blob/master/examples/pretrained_word_embeddings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os,sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'data'\n",
    "GLOVE_DIR = BASE_DIR + '/glove.6B/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '/20_newsgroup/'\n",
    "MAX_SEQ_LEN = 1000\n",
    "MAX_NB_WORDS = 20000 # top 20k most freq words\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:07, 56653.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_index = {} # maps words to its embedding vector\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in tqdm(f):\n",
    "        vals = line.split()\n",
    "        word = vals[0]\n",
    "        embedding_index[word] = np.asarray(vals[1:], dtype='float')\n",
    "print('found %d word vectors.' % len(embedding_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 87.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 19997 texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [] # text bodies\n",
    "labels_index = {} # maps label name to label id\n",
    "labels = [] # label (ids)\n",
    "for categ_name in tqdm(sorted(os.listdir(TEXT_DATA_DIR))): \n",
    "    path = os.path.join(TEXT_DATA_DIR, categ_name)\n",
    "    label_id = len(labels_index)\n",
    "    labels_index[categ_name] = label_id\n",
    "    for fname in sorted(os.listdir(path)):\n",
    "        if fname.isdigit():\n",
    "            fpath = os.path.join(path, fname)\n",
    "            with open(fpath) as f:\n",
    "                texts.append(f.read() )\n",
    "            labels.append(label_id)\n",
    "print('found %d texts' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize text into 2d integer tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "seqs = tokenizer.texts_to_sequences(texts) # turn article into list of ids\n",
    "word_index = tokenizer.word_index # maps word to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665 3990\n"
     ]
    }
   ],
   "source": [
    "print(word_index['play'], word_index['plays']) # havn't done stemming\n",
    "# but that's fine because the globe havn't done stemming too:\n",
    "# print(embedding_index['play'], embedding_index['plays'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 214909 unique tokens.\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "print('found %s unique tokens.'%len(word_index))\n",
    "print(MAX_NB_WORDS) # the MAX_NB_WORDS argument in tokenizer didn't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data tensor: (19997, 1000)\n",
      "shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "# Pads each sequence to the same length: the longest sequence.\n",
    "data = pad_sequences(sequences=seqs, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "# one-hot encoding for labels\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('shape of data tensor:', data.shape)\n",
    "print('shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data, labels = data[indices], labels[indices]\n",
    "validset_sz = int(VALIDATION_SPLIT*data.shape[0])\n",
    "X_train, Y_train = data[:-validset_sz], labels[:-validset_sz]\n",
    "X_val, Y_val = data[-validset_sz:], labels[-validset_sz:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214909/214909 [00:00<00:00, 2062388.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# embedding matrix: [vec(w) for w in dictionary]\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros( (nb_words+1, EMBEDDING_DIM) ) \n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > MAX_NB_WORDS: continue # in the seqs we only keep most freq 20k words\n",
    "    embedding_vec = embedding_index.get(word)\n",
    "    if embedding_vec is not None:\n",
    "        embedding_matrix[i,:] = embedding_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32') # instantiate a Keras tensor\n",
    "\n",
    "embedding_layer = Embedding(input_dim=nb_words+1,output_dim=EMBEDDING_DIM,\n",
    "          weights=[embedding_matrix],\n",
    "          input_length=MAX_SEQ_LEN, # Length of input sequences\n",
    "          trainable=False #keep the embeddings fixed\n",
    "         )\n",
    "embedded_seqs = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_seqs)\n",
    "x = MaxPooling1D(5)(x)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "\n",
    "# ??Flatten() gives a function ==> is this similar to Sequential model.add?\n",
    "x = Flatten()(x) \n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "preds = Dense( output_dim=len(labels_index), activation='softmax' )(x)\n",
    "\n",
    "model = Model(input=sequence_input, output=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 221s - loss: 0.5277 - acc: 0.8244 - val_loss: 0.2359 - val_acc: 0.9282\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 224s - loss: 0.1691 - acc: 0.9457 - val_loss: 0.1626 - val_acc: 0.9417\n",
      "3999/3999 [==============================] - 26s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16258775247770954, 0.94173543363489132]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n",
    "         nb_epoch=2, batch_size=128)\n",
    "# model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use `Sequential` instead of `Model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前用的是`Model`的方式构造, 需要指定input和output, 然后层与层之间用: \n",
    "\n",
    "    next_layer_out = layer_type(layer_param)(last_layer_out)\n",
    "\n",
    "的方式进行连接, 这种一堆layer构成的sequential的模型('linear stack of layers').\n",
    "\n",
    "另外这种模型完全可以用`Sequential()`代替, 只要不断`add`即可:\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add( layer_type(layer_param) ) # 1st layer need input_dim param\n",
    "    model.add( layer_type(layer_param) )\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(embedding_layer)\n",
    "model2.add(Conv1D(128, 5, activation='relu'))\n",
    "model2.add(MaxPooling1D(5))\n",
    "model2.add(Conv1D(128, 5, activation='relu'))\n",
    "model2.add(MaxPooling1D(5))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(output_dim=len(labels_index), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 219s - loss: 0.2294 - acc: 0.9356 - val_loss: 0.2068 - val_acc: 0.9342\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 223s - loss: 0.1352 - acc: 0.9563 - val_loss: 0.1831 - val_acc: 0.9420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff296947a10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n",
    "         nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上用`add`的写法还有更简单的形式: 直接在`Sequential()`里传入一个list, 里面是要添加的layer. \n",
    "\n",
    "    model = Sequential([\n",
    "        layer_type(layer_param) # 1st layer need input_dim param\n",
    "        layer_type(layer_param)\n",
    "        ...\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/2\n",
      "15998/15998 [==============================] - 225s - loss: 2.1115 - acc: 0.3087 - val_loss: 1.1575 - val_acc: 0.6084\n",
      "Epoch 2/2\n",
      "15998/15998 [==============================] - 219s - loss: 0.6927 - acc: 0.7791 - val_loss: 0.3492 - val_acc: 0.9085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2960c4c50>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential([\n",
    "        embedding_layer,\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        MaxPooling1D(5),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        MaxPooling1D(5),\n",
    "        Flatten(),\n",
    "        Dense(len(labels_index), activation='softmax')\n",
    "    ])\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics=['acc'])\n",
    "model3.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n",
    "         nb_epoch=2, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:thesis_nb]",
   "language": "python",
   "name": "conda-env-thesis_nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
