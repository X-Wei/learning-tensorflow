{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph input\n",
    "x = tf.placeholder('float', [None, n_input])\n",
    "y = tf.placeholder('float', [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variables to optimize\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes])) }\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define model: write a function to do so\n",
    "def mlp(x, weights, biases):\n",
    "    layer_1 = tf.add( tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add( tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] \n",
    "    '''\n",
    "    in fact, tf.add and the operator + are exactly the same: \n",
    "    http://stackoverflow.com/questions/37900780/in-tensorflow-what-is-the-difference-between-tf-add-and-operator\n",
    "    '''\n",
    "    return out_layer\n",
    "\n",
    "pred = mlp(x, weights, biases) # the model, pred is the logits, to add a softmax over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function: same as logistic regression, use cross entropy\n",
    "'''\n",
    "here use Adam optimizer, which is better than the GD optimizer, see: \n",
    "http://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow\n",
    "'''\n",
    "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(pred, y) )\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for evaluation\n",
    "correct_pred = tf.equal( tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "==> need to define this init AFTER all ops are defined ! \n",
    "this function initialize the variables present in the graph **when it is called**\n",
    "http://stackoverflow.com/questions/33788989/tensorflow-using-adam-optimizer\n",
    "'''\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.saver.Saver object at 0x7f70e81d2750>\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'model/mlp.ckpt'\n",
    "saver = tf.train.Saver(name='mlp')\n",
    "print saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1, cost = 159.611814384\n",
      "epoch  2, cost = 39.596413512\n",
      "epoch  3, cost = 25.040420440\n",
      "epoch  4, cost = 17.685205268\n",
      "epoch  5, cost = 12.825376380\n",
      "epoch  6, cost = 9.576980130\n",
      "epoch  7, cost = 7.225939726\n",
      "epoch  8, cost = 5.344362521\n",
      "epoch  9, cost = 4.168451068\n",
      "epoch 10, cost = 3.240559319\n",
      "epoch 11, cost = 2.440037495\n",
      "epoch 12, cost = 1.896532100\n",
      "epoch 13, cost = 1.434195745\n",
      "epoch 14, cost = 1.163079214\n",
      "epoch 15, cost = 0.883589673\n",
      "finish!\n",
      "accurracy = 0.942499995\n",
      "model is saved at model/mlp.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run( init)\n",
    "    for epoch in xrange(1, training_epochs+1):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for i in xrange(total_batch):\n",
    "            xs, ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x:xs, y:ys})\n",
    "            avg_cost += c/total_batch\n",
    "        if epoch % display_step == 0: \n",
    "            print 'epoch %2d, cost = %.9f' %(epoch, avg_cost)\n",
    "    print 'finish!'\n",
    "    print 'accurracy = %.9f' % sess.run(accuracy, \n",
    "                                        {x:mnist.test.images, y:mnist.test.labels}) # or `accuracy.eval)()`\n",
    "    \n",
    "    # save the model using saver.save()\n",
    "    save_path = saver.save(sess, MODEL_PATH)\n",
    "    print 'model is saved at %s' % save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded from file: model/mlp.ckpt\n",
      "epoch  0, cost = 0.755212577\n",
      "epoch  1, cost = 0.556708338\n",
      "epoch  2, cost = 0.546690165\n",
      "epoch  3, cost = 0.472998166\n",
      "epoch  4, cost = 0.412938899\n",
      "second optimization finished !\n",
      "accurracy = 0.944500029\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess2: \n",
    "#     sess2.run(init) \n",
    "    '''The variables to\n",
    "    restore do not have to have been initialized, as restoring is itself a way\n",
    "    to initialize variables.'''\n",
    "    saver.restore(sess2, save_path)\n",
    "    print 'model loaded from file: %s' % save_path\n",
    "    \n",
    "    # resume training\n",
    "    for epoch in xrange(5):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for i in xrange(total_batch):\n",
    "            xs, ys = mnist.train.next_batch(batch_size)\n",
    "            _,c = sess2.run([optimizer, cost], feed_dict = {x:xs, y:ys})\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0: \n",
    "            print 'epoch %2d, cost = %.9f' %(epoch, avg_cost)\n",
    "    print 'second optimization finished !'\n",
    "    print 'accurracy = %.9f' % sess2.run(accuracy, \n",
    "                                        {x:mnist.test.images, y:mnist.test.labels})"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
