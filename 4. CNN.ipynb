{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# Import MNST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000 # max training iteration\n",
    "batch_size = 128\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "p_dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph input\n",
    "x = tf.placeholder('float',[None, n_input])\n",
    "y = tf.placeholder('float',[None, n_classes])\n",
    "keep_prob = tf.placeholder('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables to use\n",
    "weights = {\n",
    "    # 5*5 conv layer, 1 input, 32 output (28+5-1=32)\n",
    "    'wc1': tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "    # 5*5 conv layer, 32 inputs, 64 output (28+5-1=32)\n",
    "    'wc2': tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "    # fully connected layer, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # output layer, 1024 inputs, 10 outputs\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model, wrapper1 -- conv2d\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    '''\n",
    "    Here is an explination of the strides parameter in conv2d: \n",
    "    http://stackoverflow.com/questions/34642595/tensorflow-strides-argument\n",
    "    >the input tensor has 4 dimensions:  [batch, height, width, channels]\n",
    "    \n",
    "    tf.nn.bias_add is almost the same as tf.add\n",
    "    https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#bias_add\n",
    "    '''\n",
    "    out = tf.nn.conv2d(input=x, filter=W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    out = tf.nn.bias_add(out, b)\n",
    "    return tf.nn.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model, wrapper2 -- max pooling\n",
    "def maxpool2d(x, k=2):\n",
    "    return tf.nn.max_pool(value=x, ksize=[1,k,k,1], strides=[1,k,k,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def convnet(x, weights, biases, dropout):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    # conv layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1)\n",
    "    # conv layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    # fully connected layer\n",
    "    fc1 = tf.reshape( conv2, \n",
    "                    shape=(-1, weights['wd1'].get_shape().as_list()[0])  )\n",
    "    fc1 = tf.matmul(fc1, weights['wd1']) + biases['bd1']\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob=dropout)\n",
    "    # output layer: output logits\n",
    "    out = tf.matmul(fc1, weights['out']) + biases['out']\n",
    "    return out\n",
    "\n",
    "pred = convnet(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal( tf.argmax(pred,1), tf.argmax(y,1) )\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1280, minibatch loss=21012.941406, accuracy=0.25781\n",
      "iter 2560, minibatch loss=12544.046875, accuracy=0.49219\n",
      "iter 3840, minibatch loss=8028.247559, accuracy=0.60156\n",
      "iter 5120, minibatch loss=4436.107910, accuracy=0.78906\n",
      "iter 6400, minibatch loss=3245.677979, accuracy=0.79688\n",
      "iter 7680, minibatch loss=7462.788574, accuracy=0.71875\n",
      "iter 8960, minibatch loss=3210.405029, accuracy=0.83594\n",
      "iter 10240, minibatch loss=2923.085938, accuracy=0.81250\n",
      "iter 11520, minibatch loss=1718.688721, accuracy=0.85938\n",
      "iter 12800, minibatch loss=3295.904053, accuracy=0.80469\n",
      "iter 14080, minibatch loss=1269.590454, accuracy=0.92969\n",
      "iter 15360, minibatch loss=1638.359985, accuracy=0.92969\n",
      "iter 16640, minibatch loss=1812.238525, accuracy=0.87500\n",
      "iter 17920, minibatch loss=1454.659912, accuracy=0.90625\n",
      "iter 19200, minibatch loss=926.651306, accuracy=0.91406\n",
      "iter 20480, minibatch loss=577.515686, accuracy=0.92188\n",
      "iter 21760, minibatch loss=2996.989014, accuracy=0.85156\n",
      "iter 23040, minibatch loss=1050.532837, accuracy=0.92969\n",
      "iter 24320, minibatch loss=1291.131592, accuracy=0.91406\n",
      "iter 25600, minibatch loss=1434.784058, accuracy=0.89062\n",
      "iter 26880, minibatch loss=1209.286621, accuracy=0.92188\n",
      "iter 28160, minibatch loss=1211.923096, accuracy=0.90625\n",
      "iter 29440, minibatch loss=1575.572998, accuracy=0.90625\n",
      "iter 30720, minibatch loss=1412.308594, accuracy=0.93750\n",
      "iter 32000, minibatch loss=1152.766357, accuracy=0.92969\n",
      "iter 33280, minibatch loss=447.932800, accuracy=0.92188\n",
      "iter 34560, minibatch loss=426.697174, accuracy=0.97656\n",
      "iter 35840, minibatch loss=264.079773, accuracy=0.94531\n",
      "iter 37120, minibatch loss=1430.735352, accuracy=0.87500\n",
      "iter 38400, minibatch loss=303.033417, accuracy=0.96875\n",
      "iter 39680, minibatch loss=309.923401, accuracy=0.96875\n",
      "iter 40960, minibatch loss=1458.517822, accuracy=0.87500\n",
      "iter 42240, minibatch loss=819.282898, accuracy=0.92188\n",
      "iter 43520, minibatch loss=703.067139, accuracy=0.95312\n",
      "iter 44800, minibatch loss=477.568817, accuracy=0.95312\n",
      "iter 46080, minibatch loss=13.621368, accuracy=0.99219\n",
      "iter 47360, minibatch loss=896.109497, accuracy=0.91406\n",
      "iter 48640, minibatch loss=1067.081543, accuracy=0.94531\n",
      "iter 49920, minibatch loss=482.877319, accuracy=0.96094\n",
      "iter 51200, minibatch loss=482.448944, accuracy=0.96094\n",
      "iter 52480, minibatch loss=590.585815, accuracy=0.92188\n",
      "iter 53760, minibatch loss=323.639069, accuracy=0.97656\n",
      "iter 55040, minibatch loss=1082.526123, accuracy=0.91406\n",
      "iter 56320, minibatch loss=110.000122, accuracy=0.97656\n",
      "iter 57600, minibatch loss=617.008057, accuracy=0.93750\n",
      "iter 58880, minibatch loss=339.057434, accuracy=0.96094\n",
      "iter 60160, minibatch loss=228.625748, accuracy=0.97656\n",
      "iter 61440, minibatch loss=712.784424, accuracy=0.92188\n",
      "iter 62720, minibatch loss=667.091797, accuracy=0.94531\n",
      "iter 64000, minibatch loss=639.934326, accuracy=0.93750\n",
      "iter 65280, minibatch loss=216.223007, accuracy=0.98438\n",
      "iter 66560, minibatch loss=501.934845, accuracy=0.94531\n",
      "iter 67840, minibatch loss=1013.487244, accuracy=0.92969\n",
      "iter 69120, minibatch loss=619.358948, accuracy=0.93750\n",
      "iter 70400, minibatch loss=551.726868, accuracy=0.97656\n",
      "iter 71680, minibatch loss=957.873474, accuracy=0.93750\n",
      "iter 72960, minibatch loss=405.671814, accuracy=0.96875\n",
      "iter 74240, minibatch loss=308.120117, accuracy=0.97656\n",
      "iter 75520, minibatch loss=382.891052, accuracy=0.96875\n",
      "iter 76800, minibatch loss=303.712219, accuracy=0.94531\n",
      "iter 78080, minibatch loss=1163.694580, accuracy=0.89062\n",
      "iter 79360, minibatch loss=72.262741, accuracy=0.98438\n",
      "iter 80640, minibatch loss=656.438110, accuracy=0.95312\n",
      "iter 81920, minibatch loss=245.272858, accuracy=0.97656\n",
      "iter 83200, minibatch loss=605.983887, accuracy=0.94531\n",
      "iter 84480, minibatch loss=583.525024, accuracy=0.93750\n",
      "iter 85760, minibatch loss=339.233582, accuracy=0.94531\n",
      "iter 87040, minibatch loss=290.225281, accuracy=0.96875\n",
      "iter 88320, minibatch loss=628.933167, accuracy=0.92969\n",
      "iter 89600, minibatch loss=202.586090, accuracy=0.96094\n",
      "iter 90880, minibatch loss=165.313263, accuracy=0.96875\n",
      "iter 92160, minibatch loss=990.068420, accuracy=0.92188\n",
      "iter 93440, minibatch loss=516.247803, accuracy=0.96094\n",
      "iter 94720, minibatch loss=829.432434, accuracy=0.94531\n",
      "iter 96000, minibatch loss=215.734558, accuracy=0.97656\n",
      "iter 97280, minibatch loss=883.707642, accuracy=0.95312\n",
      "iter 98560, minibatch loss=165.388397, accuracy=0.97656\n",
      "iter 99840, minibatch loss=289.263306, accuracy=0.97656\n",
      "iter 101120, minibatch loss=211.057281, accuracy=0.96094\n",
      "iter 102400, minibatch loss=508.475739, accuracy=0.94531\n",
      "iter 103680, minibatch loss=160.574142, accuracy=0.97656\n",
      "iter 104960, minibatch loss=897.054321, accuracy=0.91406\n",
      "iter 106240, minibatch loss=43.758011, accuracy=0.97656\n",
      "iter 107520, minibatch loss=405.654205, accuracy=0.94531\n",
      "iter 108800, minibatch loss=136.443787, accuracy=0.96875\n",
      "iter 110080, minibatch loss=250.792511, accuracy=0.96094\n",
      "iter 111360, minibatch loss=97.979683, accuracy=0.97656\n",
      "iter 112640, minibatch loss=310.741699, accuracy=0.96094\n",
      "iter 113920, minibatch loss=511.889771, accuracy=0.96875\n",
      "iter 115200, minibatch loss=240.107941, accuracy=0.96875\n",
      "iter 116480, minibatch loss=281.533630, accuracy=0.96094\n",
      "iter 117760, minibatch loss=349.903564, accuracy=0.96094\n",
      "iter 119040, minibatch loss=624.400940, accuracy=0.93750\n",
      "iter 120320, minibatch loss=252.865067, accuracy=0.96094\n",
      "iter 121600, minibatch loss=15.817261, accuracy=0.99219\n",
      "iter 122880, minibatch loss=151.732742, accuracy=0.98438\n",
      "iter 124160, minibatch loss=87.841728, accuracy=0.98438\n",
      "iter 125440, minibatch loss=532.969727, accuracy=0.94531\n",
      "iter 126720, minibatch loss=160.048355, accuracy=0.96094\n",
      "iter 128000, minibatch loss=200.105042, accuracy=0.96875\n",
      "iter 129280, minibatch loss=210.337402, accuracy=0.98438\n",
      "iter 130560, minibatch loss=115.875076, accuracy=0.97656\n",
      "iter 131840, minibatch loss=138.998749, accuracy=0.96875\n",
      "iter 133120, minibatch loss=186.517563, accuracy=0.96875\n",
      "iter 134400, minibatch loss=135.946533, accuracy=0.96875\n",
      "iter 135680, minibatch loss=268.654510, accuracy=0.96094\n",
      "iter 136960, minibatch loss=89.073563, accuracy=0.96875\n",
      "iter 138240, minibatch loss=238.222153, accuracy=0.95312\n",
      "iter 139520, minibatch loss=207.735748, accuracy=0.95312\n",
      "iter 140800, minibatch loss=155.820358, accuracy=0.97656\n",
      "iter 142080, minibatch loss=157.350769, accuracy=0.96094\n",
      "iter 143360, minibatch loss=367.723633, accuracy=0.94531\n",
      "iter 144640, minibatch loss=228.229721, accuracy=0.96875\n",
      "iter 145920, minibatch loss=304.067841, accuracy=0.96875\n",
      "iter 147200, minibatch loss=151.927734, accuracy=0.95312\n",
      "iter 148480, minibatch loss=196.723495, accuracy=0.96875\n",
      "iter 149760, minibatch loss=119.828461, accuracy=0.96875\n",
      "iter 151040, minibatch loss=613.209106, accuracy=0.93750\n",
      "iter 152320, minibatch loss=240.463928, accuracy=0.94531\n",
      "iter 153600, minibatch loss=270.715576, accuracy=0.96094\n",
      "iter 154880, minibatch loss=7.782578, accuracy=0.99219\n",
      "iter 156160, minibatch loss=441.191040, accuracy=0.94531\n",
      "iter 157440, minibatch loss=454.459991, accuracy=0.96094\n",
      "iter 158720, minibatch loss=201.799622, accuracy=0.96094\n",
      "iter 160000, minibatch loss=285.911377, accuracy=0.98438\n",
      "iter 161280, minibatch loss=185.184387, accuracy=0.96094\n",
      "iter 162560, minibatch loss=162.919128, accuracy=0.98438\n",
      "iter 163840, minibatch loss=497.142365, accuracy=0.95312\n",
      "iter 165120, minibatch loss=10.469696, accuracy=0.98438\n",
      "iter 166400, minibatch loss=107.898773, accuracy=0.96875\n",
      "iter 167680, minibatch loss=92.208710, accuracy=0.98438\n",
      "iter 168960, minibatch loss=206.559692, accuracy=0.98438\n",
      "iter 170240, minibatch loss=202.476273, accuracy=0.96094\n",
      "iter 171520, minibatch loss=152.027893, accuracy=0.96875\n",
      "iter 172800, minibatch loss=115.870384, accuracy=0.96094\n",
      "iter 174080, minibatch loss=402.600983, accuracy=0.93750\n",
      "iter 175360, minibatch loss=140.888306, accuracy=0.97656\n",
      "iter 176640, minibatch loss=79.312805, accuracy=0.96875\n",
      "iter 177920, minibatch loss=236.834778, accuracy=0.98438\n",
      "iter 179200, minibatch loss=48.523346, accuracy=0.98438\n",
      "iter 180480, minibatch loss=207.031189, accuracy=0.95312\n",
      "iter 181760, minibatch loss=57.170486, accuracy=0.97656\n",
      "iter 183040, minibatch loss=269.658844, accuracy=0.97656\n",
      "iter 184320, minibatch loss=113.610374, accuracy=0.96875\n",
      "iter 185600, minibatch loss=76.025375, accuracy=0.98438\n",
      "iter 186880, minibatch loss=53.539234, accuracy=0.98438\n",
      "iter 188160, minibatch loss=32.211914, accuracy=0.99219\n",
      "iter 189440, minibatch loss=4.655151, accuracy=0.99219\n",
      "iter 190720, minibatch loss=65.583580, accuracy=0.98438\n",
      "iter 192000, minibatch loss=40.635460, accuracy=0.99219\n",
      "iter 193280, minibatch loss=305.879242, accuracy=0.93750\n",
      "iter 194560, minibatch loss=90.810104, accuracy=0.97656\n",
      "iter 195840, minibatch loss=191.506134, accuracy=0.96094\n",
      "iter 197120, minibatch loss=261.315277, accuracy=0.96094\n",
      "iter 198400, minibatch loss=155.781342, accuracy=0.99219\n",
      "iter 199680, minibatch loss=99.710541, accuracy=0.97656\n",
      "finish!\n",
      "test set accuracy=0.980468750\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    step = 1\n",
    "    while step*batch_size<training_iters:\n",
    "        xs, ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={x:xs, y:ys, keep_prob:p_dropout})\n",
    "        if step % display_step == 0: \n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x:xs, y:ys, keep_prob:1.0}) # why 1.0 ? \n",
    "            print 'iter %d, minibatch loss=%.6f, accuracy=%.5f' % (step*batch_size, loss, acc)\n",
    "        step += 1\n",
    "    print 'finish!'\n",
    "    print 'test set accuracy=%.9f' % sess.run(accuracy, \n",
    "                                              feed_dict={x: mnist.test.images[:256],\n",
    "                                                        y: mnist.test.labels[:256],\n",
    "                                                        keep_prob: 1.0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
